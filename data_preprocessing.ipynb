{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ciso8601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import copy\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import ciso8601\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/user/Desktop/AML/thesis/data/CC_LCL-FullData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the power column and convert string to datetime and convert \"Null\" string to Nan\n",
    "df.rename(columns={'KWH/hh (per half hour) ': 'load'},inplace=True)\n",
    "df['DateTime'] = df['DateTime'].apply(lambda x:  ciso8601.parse_datetime(x[0:-8]))\n",
    "df.load = pd.to_numeric(df.load,errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13756\\2371656722.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  x = df.drop_duplicates(['LCLid','DateTime']).groupby(['LCLid']).mean()\n"
     ]
    }
   ],
   "source": [
    "x = df.drop_duplicates(['LCLid','DateTime']).groupby(['LCLid']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13756\\1387506559.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.median is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  y = df.drop_duplicates(['LCLid','DateTime']).groupby(['LCLid']).median()\n"
     ]
    }
   ],
   "source": [
    "y = df.drop_duplicates(['LCLid','DateTime']).groupby(['LCLid']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.111239230527941"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.load.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.925"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.load.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['LCLid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping only standard tariff customers\n",
    "df_std = df[df.stdorToU=='Std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding which ids have data from the entirety of the 2013\n",
    "start = datetime(year=2013, month = 1, day = 1, hour = 0, minute= 0, second= 0)\n",
    "end = datetime(year=2013, month = 12, day = 31, hour = 23, minute= 30, second= 0)\n",
    "ids = set(df_std[df_std['DateTime']==start].LCLid) & set(df_std[df_std['DateTime']==end].LCLid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping data only for the year of 2013 and for the ids that have data for the entire 2013\n",
    "df_std_2013 = df_std[(df_std.DateTime>=start)&(df_std.DateTime<=end)&(df_std.LCLid.isin(ids))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the necesarry duplicates and resetting index\n",
    "df_std_2013.drop_duplicates(['LCLid','DateTime'],keep = 'first',inplace = True)\n",
    "df_std_2013.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std_2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_std_2013.LCLid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non zero load \n",
    "ids2 = set(df_std_2013.LCLid.unique()) - set(df_std_2013[(df_std_2013.load==0)]['LCLid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std_2013 = df_std_2013[df_std_2013.LCLid.isin(ids2)]\n",
    "df_std_2013.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding those participants for which the difference between two consectuive rows' datetimes is more than an hour or less than half an hour\n",
    "names = []\n",
    "x = df_std_2013.groupby('LCLid')['DateTime'].diff()\n",
    "y = x[(x > timedelta(hours=1))|(x < timedelta(minutes = 30))]\n",
    "\n",
    "for idx in y.index:\n",
    "    names.append(df_std_2013.loc[idx,'LCLid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std_2013_cleaned = df_std_2013[~df_std_2013.LCLid.isin(set(names))]\n",
    "df_std_2013_cleaned.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting rows at appropriate locations\n",
    "# Calculate time difference between consecutive rows for each ID\n",
    "df_std_2013_cleaned['time_diff'] = df_std_2013_cleaned.groupby('LCLid')['DateTime'].diff()\n",
    "\n",
    "\n",
    "# Filter rows where time difference is greater than 30 minutes\n",
    "rows_to_insert = df_std_2013_cleaned[df_std_2013_cleaned['time_diff'] > timedelta(minutes=30)].copy()\n",
    "\n",
    "# Create new rows to be inserted\n",
    "rows_to_insert['DateTime'] = rows_to_insert['DateTime'] - timedelta(minutes=30)\n",
    "\n",
    "# Calculate the load as the average of previous and following rows\n",
    "rows_to_insert['load'] = (rows_to_insert['load'].shift() + rows_to_insert['load'].shift(-1)) / 2\n",
    "\n",
    "# Concatenate original DataFrame and rows to be inserted\n",
    "df_std_2013_cleaned = pd.concat([df_std_2013_cleaned, rows_to_insert]).sort_values(['LCLid', 'DateTime']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill load Nans with average of the previous and latter. It was checked that for no id, the first or the last load value is Nan. Thus, no mixup between IDs\n",
    "# Also dropping the time_diff column\n",
    "df_std_2013_cleaned['load'] = df_std_2013_cleaned['load'].fillna((df_std_2013_cleaned['load'].ffill() + df_std_2013_cleaned['load'].bfill()) / 2)\n",
    "df_std_2013_cleaned.drop('time_diff', axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows that ony time diff is 30 mins\n",
    "diff = df_std_2013_cleaned.groupby(['LCLid']).DateTime.diff()\n",
    "set(diff[~(diff==timedelta(minutes = 30))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acorn = pd.read_csv(\"C:/Users/user/Desktop/AML/thesis/data/UK power networks data/informations_households.csv\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the Acorn column\n",
    "df_merged = df_std_2013_cleaned.merge(df_acorn[['LCLid', 'Acorn']], on='LCLid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.groupby(['Acorn','LCLid']).count().groupby(['Acorn']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting all ids for a given Acorn Group\n",
    "df_id_acorn = df_merged[['LCLid','Acorn']].drop_duplicates(['LCLid','Acorn'])\n",
    "acorns = df_id_acorn['Acorn'].unique()\n",
    "acorns.sort()\n",
    "dict = {}\n",
    "for i in acorns:\n",
    "    dict[i] = df_id_acorn[df_id_acorn.Acorn==i].LCLid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sampling 45% per group \n",
    "# random.seed(0)\n",
    "# final_ids = []\n",
    "# for i in dict:\n",
    "#     samples = np.random.choice(dict[i], size=int(0.45*len(dict[i])), replace=False)\n",
    "#     final_ids.extend(samples.tolist())\n",
    "# final_ids.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ids = df_id_acorn.LCLid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(final_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids = df_merged[df_merged.LCLid.isin(final_ids)].sort_values(['LCLid', 'DateTime', 'Acorn'], ascending=[True, True, True]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store the aggregated rows\n",
    "df_aggregated = pd.DataFrame()\n",
    "\n",
    "# Assign the LCLid, stdorToU, and DateTime from the first row of every two rows\n",
    "df_aggregated['LCLid'] = df_ids['LCLid'].iloc[::2].values\n",
    "df_aggregated['stdorToU'] = df_ids['stdorToU'].iloc[::2].values\n",
    "df_aggregated['DateTime'] = df_ids['DateTime'].iloc[::2].values\n",
    "\n",
    "# Combine the Load values of every two consecutive rows\n",
    "df_aggregated['load'] = df_ids['load'].iloc[::2].values + df_ids['load'].iloc[1::2].values\n",
    "\n",
    "#Assign the Acorn from the first row of every two rows\n",
    "df_aggregated['Acorn'] = df_ids['Acorn'].iloc[::2].values\n",
    "\n",
    "df_aggregated.sort_values(['LCLid', 'DateTime', 'Acorn'], ascending=[True, True, True]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated.groupby(['LCLid','Acorn']).count().groupby(['Acorn']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows that ony time diff is 1 hour\n",
    "diff = df_aggregated.groupby(['LCLid']).DateTime.diff()\n",
    "set(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create a datetime object\n",
    "dt = datetime.now()\n",
    "\n",
    "# Get the day of the week (Monday is 0 and Sunday is 6)\n",
    "day_of_week = dt.weekday()\n",
    "\n",
    "# Get the hour in 24-hour format\n",
    "hour = dt.strftime(\"%H\")\n",
    "\n",
    "# Print the day of the week and hour\n",
    "print(\"Day of the week:\", day_of_week)\n",
    "print(\"Hour:\", hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the day of the week and hour column\n",
    "df_aggregated['Weekday'] = df_aggregated.DateTime.apply(lambda x: x.weekday())\n",
    "df_aggregated['Hour'] = df_aggregated.DateTime.apply(lambda x: int(x.strftime(\"%H\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv('weather_hourly_darksky.csv')\n",
    "df_weather.rename(columns={'time': 'DateTime'},inplace=True)\n",
    "df_weather['DateTime'] = df_weather['DateTime'].apply(lambda x:  ciso8601.parse_datetime(x))\n",
    "df_weather.sort_values(['DateTime'],inplace = True)\n",
    "df_weather.reset_index(drop = True,inplace = True)\n",
    "df_weather.head(2)\n",
    "\n",
    "idx = df_weather[df_weather['DateTime'].diff()==timedelta(hours = 3)].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather = pd.read_csv('weather_hourly_darksky.csv')\n",
    "df_weather.rename(columns={'time': 'DateTime'},inplace=True)\n",
    "df_weather['DateTime'] = df_weather['DateTime'].apply(lambda x:  ciso8601.parse_datetime(x))\n",
    "df_weather.sort_values(['DateTime'],inplace = True)\n",
    "df_weather.reset_index(drop = True,inplace = True)\n",
    "df_weather.head(2)\n",
    "\n",
    "idx = df_weather[df_weather['DateTime'].diff()==timedelta(hours = 3)].index[0]\n",
    "print(idx)\n",
    "\n",
    "\n",
    "#inserting missing rows. only one pair of consecutive rows for which diff is more than 1 (3hrs). same apparent temp\n",
    "# Create the new DataFrame with new rows\n",
    "row1 = copy.deepcopy(df_weather.iloc[idx-1])\n",
    "row1['DateTime'] = row1['DateTime'] + timedelta(hours = 1)\n",
    "row2 = copy.deepcopy(df_weather.iloc[idx-1])\n",
    "row2['DateTime'] = row2['DateTime'] + timedelta(hours = 2)\n",
    "\n",
    "# Concatenate the new rows with the original DataFrame and sort the index\n",
    "df_weather = pd.concat([df_weather.iloc[:idx], pd.DataFrame([row1]),pd.DataFrame([row2]), df_weather.iloc[idx:]]).reset_index(drop=True)\n",
    "df_weather.sort_values(['DateTime'],inplace = True)\n",
    "df_weather.reset_index(drop = True,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding apparent temperature\n",
    "df_aggregated = pd.merge(df_aggregated, df_weather[['DateTime','apparentTemperature']], on='DateTime', how='left')\n",
    "df_aggregated.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Group the dataframe by LCLid and weekday/weekend category\n",
    "grouped = df_aggregated.groupby(['LCLid', df_aggregated['DateTime'].dt.weekday.isin([5, 6])])\n",
    "\n",
    "# Step 4: Create a new column with default values for AVG4D\n",
    "df_aggregated['AVG4D'] = pd.Series(dtype='float64')\n",
    "\n",
    "# Step 5: Define a function to calculate AVG4D using rolling window\n",
    "def calculate_avg4d(group):\n",
    "    # Identify current day's category\n",
    "    is_weekend = group.name[1]\n",
    "\n",
    "    # Calculate rolling average load over a window of 4 days\n",
    "    rolling_avg = group['load'].rolling(window=4, min_periods=1).mean()\n",
    "    \n",
    "    # Assign rolling average values to AVG4D column\n",
    "    group['AVG4D'] = rolling_avg\n",
    "    \n",
    "    return group\n",
    "\n",
    "# Step 6: Apply the function to each group\n",
    "grouped.apply(calculate_avg4d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weather.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sort the dataframe by DateTime in ascending order\n",
    "df_aggregated = df_aggregated.sort_values('DateTime')\n",
    "\n",
    "# Create a boolean mask to identify weekdays and weekends\n",
    "is_weekday = ~(df_aggregated['DateTime'].dt.dayofweek.isin([5,6]))\n",
    "\n",
    "# Create rolling windows for weekdays and weekends, considering the same time and LCLid\n",
    "weekday_window = df_aggregated[is_weekday].groupby(['LCLid', df_aggregated['DateTime'].dt.time])['load'].rolling(4,min_periods = 1)\n",
    "weekend_window = df_aggregated[~is_weekday].groupby(['LCLid', df_aggregated['DateTime'].dt.time])['load'].rolling(4,min_periods = 1)\n",
    "\n",
    "# Compute the average load for each rolling window\n",
    "df_aggregated['AVG4D'] = np.nan\n",
    "df_aggregated.loc[is_weekday, 'AVG4D'] = weekday_window.mean().reset_index(level=[0, 1], drop=True)\n",
    "df_aggregated.loc[~is_weekday, 'AVG4D'] = weekend_window.mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "df_aggregated.sort_values(['LCLid', 'DateTime', 'Acorn'], ascending=[True, True, True],inplace = True)\n",
    "df_aggregated.reset_index(drop = True,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_aggregated['LCLid'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempcluster\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a sample series\n",
    "series = df_aggregated['apparentTemperature']\n",
    "\n",
    "# Reshape the series to a 2D array\n",
    "X = series.values.reshape(-1, 1)\n",
    "\n",
    "# Apply K-means clustering with K = 2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster labels for each point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Print the cluster labels\n",
    "print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aggregated['TempCluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('final_ids.pkl', 'wb') as f:\n",
    "#     pickle.dump(final_ids, f)  # Save ids to a pickle file\n",
    "\n",
    "# with open('df_final.pkl', 'wb') as f:\n",
    "#     pickle.dump(df_final, f)  # Save DataFrame to a pickle file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
